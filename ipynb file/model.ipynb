{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Data set 1 means undersampling set, data set 2 means oversampling set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import seaborn\n",
    "import pickle\n",
    "from sklearn.cross_validation import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"C:\\\\Users\\\\谢婷谢婷\\\\Desktop\\\\creditcard\\\\train_test.data\", 'rb')\n",
    "# load data\n",
    "dataset = pickle.load(f)\n",
    "#generating variables\n",
    "X_test1=dataset[0]\n",
    "X_test2=dataset[1]\n",
    "X_train1=dataset[2]\n",
    "X_train2=dataset[3]\n",
    "y_test1=dataset[4]\n",
    "y_test2=dataset[5]\n",
    "y_train1=dataset[6]\n",
    "y_train2=dataset[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[3]:\n",
    "# define plot confusion matrix function\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,precision_recall_curve,auc,roc_auc_score,roc_curve,recall_score,classification_report \n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=0)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        #print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        1#print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "# In[4]:\n",
    "# Logistic Regression\n",
    "    \n",
    "# cross validation to choose parameters in Logistic Regression\n",
    "def printing_Kfold_scores(x_train_data,y_train_data):\n",
    "    fold = KFold(len(y_train_data),5,shuffle=False) \n",
    "\n",
    "    # Different C parameters\n",
    "    c_param_range = [0.01,0.1,1,10,100]\n",
    "\n",
    "    results_table = pd.DataFrame(index = range(len(c_param_range),2), columns = ['C_parameter','Mean recall score'])\n",
    "    results_table['C_parameter'] = c_param_range\n",
    "\n",
    "    # the k-fold will give 2 lists: train_indices = indices[0], test_indices = indices[1]\n",
    "    j = 0\n",
    "    for c_param in c_param_range:\n",
    "        print('-------------------------------------------')\n",
    "        print('C parameter: ', c_param)\n",
    "        print('-------------------------------------------')\n",
    "        print('')\n",
    "\n",
    "        recall_accs = []\n",
    "        for iteration, indices in enumerate(fold,start=1):\n",
    "\n",
    "            # Call the logistic regression model with a certain C parameter\n",
    "            lr = LogisticRegression(C = c_param, penalty = 'l1')\n",
    "\n",
    "            # Use the training data to fit the model. In this case, we use the portion of the fold to train the model\n",
    "            # with indices[0]. We then predict on the portion assigned as the 'test cross validation' with indices[1]\n",
    "            lr.fit(x_train_data.iloc[indices[0],:],y_train_data.iloc[indices[0],:].values.ravel())\n",
    "\n",
    "            # Predict values using the test indices in the training data\n",
    "            y_pred_undersample = lr.predict(x_train_data.iloc[indices[1],:].values)\n",
    "\n",
    "            # Calculate the recall score and append it to a list for recall scores representing the current c_parameter\n",
    "            recall_acc = recall_score(y_train_data.iloc[indices[1],:].values,y_pred_undersample)\n",
    "            recall_accs.append(recall_acc)\n",
    "            print('Iteration ', iteration,': recall score = ', recall_acc)\n",
    "\n",
    "        # The mean value of those recall scores is the metric we want to save and get hold of.\n",
    "        results_table.ix[j,'Mean recall score'] = np.mean(recall_accs)\n",
    "        j += 1\n",
    "        print('')\n",
    "        print('Mean recall score ', np.mean(recall_accs))\n",
    "        print('')\n",
    "\n",
    "    best_c = results_table.loc[results_table['Mean recall score'].idxmax()]['C_parameter']\n",
    "    \n",
    "    # Finally, we can check which C parameter is the best amongst the chosen.\n",
    "    print('*********************************************************************************')\n",
    "    print('Best model to choose from cross validation is with C parameter = ', best_c)\n",
    "    print('*********************************************************************************')\n",
    "    \n",
    "    return best_c\n",
    "\n",
    "best_c = printing_Kfold_scores(X_train1,y_train1)\n",
    "\n",
    "# In[]\n",
    "\n",
    "##########   Finishing choosing parameters\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Use this C_parameter to build the final model with the whole training dataset and predict the classes in the test\n",
    "lr = LogisticRegression(C = 0.01, penalty = 'l1')\n",
    "lr.fit(X_train1,y_train1.values.ravel())\n",
    "y_pred1 = lr.predict(X_test1.values)\n",
    "\n",
    "lr.score(X_test1, y_test1)\n",
    "print('accuracy of training set1: {:.4f}'.format(lr.score(X_train1,y_train1)))\n",
    "print('accuaracy of test set1: {:.4f}'.format(lr.score(X_test1, y_test1)))\n",
    "#accuracy calculation\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test1,y_pred1)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "print(\"Recall metric in the testing dataset: \", cnf_matrix[1,1]/(cnf_matrix[1,0]+cnf_matrix[1,1]))\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "class_names = [0,1]\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix\n",
    "                      , classes=class_names\n",
    "                      , title='Confusion matrix')\n",
    "plt.show()\n",
    "\n",
    "# ROC CURVE\n",
    "\n",
    "y_pred1_score = lr.fit(X_train1,y_train1.values.ravel()).decision_function(X_test1.values)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test1.values.ravel(),y_pred1_score)\n",
    "roc_auc = auc(fpr,tpr)\n",
    "\n",
    "# Plot ROC\n",
    "plt.title('Receiver Operating Characteristic_undersampling')\n",
    "plt.plot(fpr, tpr, 'b',label='AUC = %0.2f'% roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.xlim([-0.1,1.0])\n",
    "plt.ylim([-0.1,1.01])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()\n",
    "\n",
    "##  Undersampling finished\n",
    "\n",
    "lr = LogisticRegression(C = 0.01, penalty = 'l1')\n",
    "lr.fit(X_train2,y_train2.values.ravel())\n",
    "y_pred2 = lr.predict(X_test2.values)\n",
    "\n",
    "lr.score(X_test2, y_test2)\n",
    "print('accuracy of training set2: {:.4f}'.format(lr.score(X_train2,y_train2)))\n",
    "print('accuaracy of test set2: {:.4f}'.format(lr.score(X_test2, y_test2)))\n",
    "#accuracy calculation\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test2,y_pred2)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "print(\"Recall metric in the testing dataset: \", cnf_matrix[1,1]/(cnf_matrix[1,0]+cnf_matrix[1,1]))\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "class_names = [0,1]\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix\n",
    "                      , classes=class_names\n",
    "                      , title='Confusion matrix')\n",
    "plt.show()\n",
    "\n",
    "# ROC CURVE\n",
    "y_pred2_score = lr.fit(X_train2,y_train2.values.ravel()).decision_function(X_test2.values)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test2.values.ravel(),y_pred2_score)\n",
    "roc_auc = auc(fpr,tpr)\n",
    "\n",
    "# Plot ROC\n",
    "plt.title('Receiver Operating Characteristic_Oversampling')\n",
    "plt.plot(fpr, tpr, 'b',label='AUC = %0.2f'% roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.xlim([-0.1,1.0])\n",
    "plt.ylim([-0.1,1.01])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knc = KNeighborsClassifier(n_neighbors = 3)\n",
    "knc.fit(X_train1,y_train1)\n",
    "y_knc = knc.predict(X_test1)\n",
    "print('accuracy of training set1: {:.4f}'.format(knc.score(X_train1,y_train1)))\n",
    "print('accuracy of test set1: {:.4f}'.format(knc.score(X_test1, y_test1)))\n",
    "#when choosing n_neighbors=3,we have the maximum acuracy of test set1\n",
    "\n",
    "knc = KNeighborsClassifier(n_neighbors = 3)\n",
    "knc.fit(X_train2,y_train2)\n",
    "y_knc = knc.predict(X_test2)\n",
    "print('accuracy of training set2: {:.4f}'.format(knc.score(X_train2,y_train2)))\n",
    "print('accuracy of test set2: {:.4f}'.format(knc.score(X_test2, y_test2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM\n",
    "# Scikit-learn library: For SVM\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import svm\n",
    "import itertools\n",
    "\n",
    "classifier = svm.SVC(kernel='linear') \n",
    "classifier.fit(X_train1, y_train1)\n",
    "prediction_SVM_all1 = classifier.predict(X_test1)\n",
    "cm1 = confusion_matrix(y_test1, prediction_SVM_all1)\n",
    "print('We have detected ' + str(cm1[1][1]) + ' frauds / ' + str(cm1[1][1]+cm1[1][0]) + ' total frauds.')\n",
    "print('\\nSo, the probability to detect a fraud is ' + str(cm1[1][1]/(cm1[1][1]+cm1[1][0])))\n",
    "print(\"the accuracy is : \"+str((cm1[0][0]+cm1[1][1]) / (sum(cm1[0]) + sum(cm1[1]))))\n",
    "#So, the probability to detect a fraud is 0.89 the accuracy is : 0.9390862944162437\n",
    "\n",
    "\n",
    "classifier = svm.SVC(kernel='linear') \n",
    "classifier.fit(X_train2, y_train2)\n",
    "prediction_SVM_all2 = classifier.predict(X_test2)\n",
    "cm2 = confusion_matrix(y_test2, prediction_SVM_all2)\n",
    "print('We have detected ' + str(cm2[1][1]) + ' frauds / ' + str(cm2[1][1]+cm1[1][0]) + ' total frauds.')\n",
    "print('\\nSo, the probability to detect a fraud is ' + str(cm2[1][1]/(cm1[1][1]+cm2[1][0])))\n",
    "print(\"the accuracy is : \"+str((cm2[0][0]+cm2[1][1]) / (sum(cm2[0]) + sum(cm2[1]))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
